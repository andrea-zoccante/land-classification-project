{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/EuroSAT/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset and Selection of a Small Subset for Few-Shot Learning\n",
    "\n",
    "### Selecting a Small Subset\n",
    "For fine-tuning, we will use a small, balanced subset of the EuroSAT dataset. Instead of using the full dataset, we will randomly sample 10 images per class to simulate a **few-shot learning** scenario. The goal is to test whether training on a small number of images can improve classification performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"2750/\"\n",
    "\n",
    "classes = sorted([cls for cls in os.listdir(DATASET_PATH) if not cls.startswith('.')])\n",
    "\n",
    "print(f\"Classes found: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annual Crop',\n",
       " 'Forest',\n",
       " 'Herbaceous Vegetation',\n",
       " 'Highway',\n",
       " 'Industrial',\n",
       " 'Pasture',\n",
       " 'Permanent Crop',\n",
       " 'Residential',\n",
       " 'River',\n",
       " 'Sea or Lake']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSNAME_DICT = {'AnnualCrop': 'Annual Crop', \n",
    "                    'Forest': 'Forest', \n",
    "                    'HerbaceousVegetation': 'Herbaceous Vegetation', \n",
    "                    'Highway': 'Highway', \n",
    "                    'Industrial': 'Industrial', \n",
    "                    'Pasture': 'Pasture', \n",
    "                    'PermanentCrop': 'Permanent Crop', \n",
    "                    'Residential': 'Residential', 'River': 'River', \n",
    "                    'SeaLake': 'Sea or Lake'}\n",
    "modified_classes = [CLASSNAME_DICT[c] for c in classes]\n",
    "modified_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"2750/\"\n",
    "\n",
    "classes = sorted([cls for cls in os.listdir(DATASET_PATH) if not cls.startswith('.')])\n",
    "\n",
    "print(f\"Classes found: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_few_shot_data(dataset_path, classes, num_samples=10):\n",
    "    \"\"\"\n",
    "    Randomly sample a small, balanced dataset for few-shot learning.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_path (str): Path to the dataset directory.\n",
    "        classes (list): List of class names.\n",
    "        num_samples (int): Number of images per class to sample.\n",
    "    \n",
    "    Returns:\n",
    "        sampled_data (list): List of (image_path, class_name) tuples.\n",
    "    \"\"\"\n",
    "    sampled_data = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        image_paths = glob(os.path.join(dataset_path, cls, \"*.jpg\"))\n",
    "        sampled_images = random.sample(image_paths, num_samples)\n",
    "        sampled_data.extend([(img, cls) for img in sampled_images])\n",
    "    \n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Path</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2750/AnnualCrop/AnnualCrop_57.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2750/AnnualCrop/AnnualCrop_1983.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2750/AnnualCrop/AnnualCrop_572.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2750/AnnualCrop/AnnualCrop_1695.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2750/AnnualCrop/AnnualCrop_1518.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2750/SeaLake/SeaLake_2428.jpg</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2750/SeaLake/SeaLake_1131.jpg</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2750/SeaLake/SeaLake_217.jpg</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2750/SeaLake/SeaLake_2152.jpg</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2750/SeaLake/SeaLake_2782.jpg</td>\n",
       "      <td>SeaLake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Image Path       Class\n",
       "0     2750/AnnualCrop/AnnualCrop_57.jpg  AnnualCrop\n",
       "1   2750/AnnualCrop/AnnualCrop_1983.jpg  AnnualCrop\n",
       "2    2750/AnnualCrop/AnnualCrop_572.jpg  AnnualCrop\n",
       "3   2750/AnnualCrop/AnnualCrop_1695.jpg  AnnualCrop\n",
       "4   2750/AnnualCrop/AnnualCrop_1518.jpg  AnnualCrop\n",
       "..                                  ...         ...\n",
       "95        2750/SeaLake/SeaLake_2428.jpg     SeaLake\n",
       "96        2750/SeaLake/SeaLake_1131.jpg     SeaLake\n",
       "97         2750/SeaLake/SeaLake_217.jpg     SeaLake\n",
       "98        2750/SeaLake/SeaLake_2152.jpg     SeaLake\n",
       "99        2750/SeaLake/SeaLake_2782.jpg     SeaLake\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_data = sample_few_shot_data(DATASET_PATH, classes, num_samples=10)\n",
    "\n",
    "few_shot_df = pd.DataFrame(few_shot_data, columns=[\"Image Path\", \"Class\"])\n",
    "few_shot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded and backbone frozen.\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)\n",
    "\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"CLIP model loaded and backbone frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=8, ctx_init=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_cls = len(classnames)\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx_dim = clip_model.text_model.final_layer_norm.weight.shape[0]\n",
    "        dtype = clip_model.dtype\n",
    "\n",
    "        # Initialize context vectors\n",
    "        if ctx_init:\n",
    "            # Initialize with provided context string\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            print(\"Initializing class-specific contexts\")\n",
    "            ctx_vectors = torch.empty(self.n_cls, n_ctx, self.ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized\n",
    "\n",
    "        # Prepare the prompts\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        print(prompts)\n",
    "        # Tokenize and get embeddings for class names\n",
    "        tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = tokenized_prompts[\"input_ids\"]\n",
    "            embedding = clip_model.text_model.embeddings.token_embedding(input_ids).type(dtype)\n",
    "\n",
    "        # Store prefix and suffix\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # Start of sequence token\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1:, :])  # Class names and rest\n",
    "\n",
    "    def forward(self):\n",
    "        # The context vectors\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # Expand context to be per-class\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        # Concatenate the prefix, context, and suffix\n",
    "        prompts = torch.cat([self.token_prefix, ctx, self.token_suffix], dim=1)\n",
    "        return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(prompt_learner, clip_model):\n",
    "    prompts = prompt_learner()  # Forward pass through prompt learner\n",
    "    text_features = clip_model.text_model(prompts).last_hidden_state  # Extract features\n",
    "    return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_indices(class_list, class_names):\n",
    "    \"\"\" Convert class names to numerical labels. \"\"\"\n",
    "    return torch.tensor([class_names.index(cls) for cls in class_list], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m labels \u001b[38;5;241m=\u001b[39m get_class_indices(few_shot_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), classes)\n\u001b[0;32m----> 2\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "labels = get_class_indices(few_shot_df[\"Class\"].tolist(), classes)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [Image.open(img_path).convert(\"RGB\") for img_path in few_shot_df[\"Image Path\"].tolist()]\n",
    "inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "prompt_learner = SimplePromptLearner(clip_model, modified_classes, n_ctx=8, ctx_init=None)\n",
    "\n",
    "optimizer = optim.Adam(prompt_learner.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "num_epochs=100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get updated text features using learned prompts\n",
    "    text_features = get_text_features(prompt_learner, clip_model)\n",
    "    \n",
    "    # Get image features\n",
    "    image_features = clip_model.encode_image(images)\n",
    "\n",
    "    # Normalize features\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    logits = image_features @ text_features.T\n",
    "    loss = criterion(logits, labels)  # Some classification loss like CrossEntropyLoss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EuroSAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
