{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import csv\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset and Selection of a Small Subset for Few-Shot Learning\n",
    "\n",
    "### Selecting a Small Subset\n",
    "For fine-tuning, we will use a small, balanced subset of the EuroSAT dataset. Instead of using the full dataset, we will randomly sample 10 images per class to simulate a **few-shot learning** scenario. The goal is to test whether training on a small number of images can improve classification performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found:  ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n"
     ]
    }
   ],
   "source": [
    "classes = config.CLASSES\n",
    "print(\"Classes found: \", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_indices(class_list, class_names):\n",
    "    \"\"\" Convert class names to numerical labels. \"\"\"\n",
    "    return torch.tensor([class_names.index(cls) for cls in class_list], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][\"Image Path\"]\n",
    "        label = self.dataframe.iloc[idx][\"Class\"]\n",
    "        label_tensor = torch.tensor([label], dtype=torch.long).to(device)\n",
    "        \n",
    "        return img_path, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_few_shot_data(dataset_path, classes, num_samples):\n",
    "    sampled_data = []\n",
    "    for cls in classes:\n",
    "        image_paths = glob(os.path.join(dataset_path, cls, \"*.jpg\"))\n",
    "        sampled_images = random.sample(image_paths, num_samples)\n",
    "        sampled_data.extend([(img, cls) for img in sampled_images])\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset and dataloader\n",
    "test_df = pd.read_csv(\"test_data/test_set.csv\")\n",
    "\n",
    "test_dataset = FewShotDataset(dataframe=test_df, image_dir=\"2750\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=60)  # Adjust batch_size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded and backbone frozen.\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.to(device)\n",
    "\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"CLIP model loaded and backbone frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.text_model\n",
    "        self.encoder = clip_model.text_model.encoder\n",
    "        self.positional_embedding = clip_model.text_model.embeddings.position_embedding\n",
    "        self.ln_final = clip_model.text_model.final_layer_norm\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        seq_length = prompts.size(1)  # Sequence length (number of tokens)\n",
    "        positions = torch.arange(seq_length, device=prompts.device).unsqueeze(0)  # Shape [1, seq_length]\n",
    "        \n",
    "        # Get the positional embeddings for each position\n",
    "        pos_embeddings = self.positional_embedding(positions)  # Shape [1, seq_length, embedding_dim]\n",
    "        \n",
    "        x = prompts + pos_embeddings\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # NLD -> LND\n",
    "        encoder_outputs = self.encoder(x)\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.ln_final(pooled_output)\n",
    "\n",
    "        text_features = self.text_projection(pooled_output)\n",
    "\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=8, ctx_init=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_cls = len(classnames)\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx_dim = clip_model.text_model.final_layer_norm.weight.shape[0]\n",
    "        dtype = clip_model.dtype\n",
    "\n",
    "        # Initialize context vectors\n",
    "        if ctx_init:\n",
    "            # Initialize with provided context string\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # print(\"Initializing class-specific contexts\")\n",
    "            ctx_vectors = torch.empty(self.n_cls, n_ctx, self.ctx_dim, dtype=dtype).to(device)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        # print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        # print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors, requires_grad=True).to(device)  # to be optimized\n",
    "\n",
    "        # Prepare the prompts\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        # Tokenize and get embeddings for class names\n",
    "        tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\", truncation=True, max_length=77).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = tokenized_prompts[\"input_ids\"]\n",
    "            embedding = clip_model.text_model.embeddings.token_embedding(input_ids).type(dtype)\n",
    "\n",
    "        # Store prefix and suffix\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # Start of sequence token\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx:, :])  # Class names and rest\n",
    "\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self):\n",
    "        # The context vectors\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # Concatenate the prefix, context, and suffix\n",
    "        prompts = torch.cat([self.token_prefix, ctx, self.token_suffix], dim=1)\n",
    "\n",
    "        return prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv(\"validation_data/validation_set.csv\")\n",
    "\n",
    "validation_labels = get_class_indices(validation_df[\"Class\"].tolist(), classes).to(device)\n",
    "\n",
    "validation_images = [Image.open(img_path).convert(\"RGB\") for img_path in validation_df[\"Image Path\"].tolist()]\n",
    "validation_inputs = clip_processor(images=validation_images, return_tensors=\"pt\", padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-shot Epoch [0/500]:\n",
      "\tTraining Loss: 4.7444\n",
      "\tValidation Loss: 4.7915\n",
      "\tValidation Accuracy: 11.20%\n",
      "\n",
      "1-shot Epoch [250/500]:\n",
      "\tTraining Loss: 0.0007\n",
      "\tValidation Loss: 2.4287\n",
      "\tValidation Accuracy: 44.00%\n",
      "\n",
      "1-shot Epoch [500/500]:\n",
      "\tTraining Loss: 0.0003\n",
      "\tValidation Loss: 2.4757\n",
      "\tValidation Accuracy: 45.20%\n",
      "\n",
      "Prompt learner for 1-shot saved to saved_prompt_learners/prompt_learner_1_shot.pth\n",
      "2-shot Epoch [0/500]:\n",
      "\tTraining Loss: 3.0558\n",
      "\tValidation Loss: 4.8715\n",
      "\tValidation Accuracy: 20.20%\n",
      "\n",
      "2-shot Epoch [250/500]:\n",
      "\tTraining Loss: 0.0004\n",
      "\tValidation Loss: 1.2689\n",
      "\tValidation Accuracy: 67.20%\n",
      "\n",
      "2-shot Epoch [500/500]:\n",
      "\tTraining Loss: 0.0002\n",
      "\tValidation Loss: 1.2730\n",
      "\tValidation Accuracy: 67.40%\n",
      "\n",
      "Prompt learner for 2-shot saved to saved_prompt_learners/prompt_learner_2_shot.pth\n",
      "4-shot Epoch [0/500]:\n",
      "\tTraining Loss: 3.9506\n",
      "\tValidation Loss: 3.7038\n",
      "\tValidation Accuracy: 14.40%\n",
      "\n",
      "4-shot Epoch [250/500]:\n",
      "\tTraining Loss: 0.0012\n",
      "\tValidation Loss: 0.9381\n",
      "\tValidation Accuracy: 74.80%\n",
      "\n",
      "4-shot Epoch [500/500]:\n",
      "\tTraining Loss: 0.0005\n",
      "\tValidation Loss: 0.9504\n",
      "\tValidation Accuracy: 75.00%\n",
      "\n",
      "Prompt learner for 4-shot saved to saved_prompt_learners/prompt_learner_4_shot.pth\n",
      "8-shot Epoch [0/500]:\n",
      "\tTraining Loss: 4.5220\n",
      "\tValidation Loss: 3.6592\n",
      "\tValidation Accuracy: 18.00%\n",
      "\n",
      "8-shot Epoch [250/500]:\n",
      "\tTraining Loss: 0.0031\n",
      "\tValidation Loss: 0.6160\n",
      "\tValidation Accuracy: 80.40%\n",
      "\n",
      "8-shot Epoch [500/500]:\n",
      "\tTraining Loss: 0.0012\n",
      "\tValidation Loss: 0.6247\n",
      "\tValidation Accuracy: 81.40%\n",
      "\n",
      "Prompt learner for 8-shot saved to saved_prompt_learners/prompt_learner_8_shot.pth\n",
      "16-shot Epoch [0/500]:\n",
      "\tTraining Loss: 3.4848\n",
      "\tValidation Loss: 5.3089\n",
      "\tValidation Accuracy: 10.00%\n",
      "\n",
      "16-shot Epoch [250/500]:\n",
      "\tTraining Loss: 0.0067\n",
      "\tValidation Loss: 0.4894\n",
      "\tValidation Accuracy: 83.60%\n",
      "\n",
      "16-shot Epoch [500/500]:\n",
      "\tTraining Loss: 0.0028\n",
      "\tValidation Loss: 0.5040\n",
      "\tValidation Accuracy: 83.80%\n",
      "\n",
      "Prompt learner for 16-shot saved to saved_prompt_learners/prompt_learner_16_shot.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs=500\n",
    "\n",
    "save_dir = \"saved_prompt_learners\"\n",
    "\n",
    "losses = {}\n",
    "validation_losses = {}\n",
    "validation_accuracies = {}\n",
    "\n",
    "for few_shot in [1,2,4,8,16]:\n",
    "    few_shot_df = pd.read_csv(f\"few_shot_data/few_shot_{few_shot}.csv\")\n",
    "    few_shot_images = [Image.open(img_path).convert(\"RGB\") for img_path in few_shot_df[\"Image Path\"].tolist()]\n",
    "    inputs = clip_processor(images=few_shot_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    labels = get_class_indices(few_shot_df[\"Class\"].tolist(), classes)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    prompt_learner = SimplePromptLearner(clip_model, modified_classes, n_ctx=16, ctx_init=None).to(device)\n",
    "    text_encoder = TextEncoder(clip_model=clip_model).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(prompt_learner.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses[few_shot] = []\n",
    "    validation_losses[few_shot] = []\n",
    "    validation_accuracies[few_shot] = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        prompts = prompt_learner()\n",
    "        \n",
    "        # Get updated text features using learned prompts, and image features\n",
    "        text_features = text_encoder(prompts)\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "        # Normalize features\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        logit_scale = clip_model.logit_scale.exp()\n",
    "        logits = image_features @ text_features.T\n",
    "        logits *= logit_scale\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses[few_shot].append(loss.item())\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                prompts = prompt_learner()\n",
    "\n",
    "                text_features = text_encoder(prompts)\n",
    "                image_features = clip_model.get_image_features(**validation_inputs)\n",
    "\n",
    "                # Normalize features\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                # Compute cosine similarity\n",
    "                logit_scale = clip_model.logit_scale.exp()\n",
    "                val_logits = image_features @ text_features.T\n",
    "                val_logits *= logit_scale\n",
    "\n",
    "                val_loss = criterion(val_logits, validation_labels)\n",
    "                \n",
    "                predictions = torch.argmax(val_logits, dim=1)\n",
    "            \n",
    "            accuracy = (predictions == validation_labels).float().mean().item()\n",
    "            \n",
    "            validation_accuracies[few_shot].append(accuracy)\n",
    "            validation_losses[few_shot].append((epoch, val_loss.item()))\n",
    "            \n",
    "            if epoch % 250 == 0:    \n",
    "                print(f\"{few_shot}-shot Epoch [{epoch}/{num_epochs}]:\") \n",
    "                print(f\"\\tTraining Loss: {loss.item():.4f}\")\n",
    "                print(f\"\\tValidation Loss: {val_loss.item():.4f}\")\n",
    "                print(f\"\\tValidation Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"prompt_learner_{few_shot}_shot.pth\")\n",
    "    torch.save(prompt_learner.state_dict(), save_path)\n",
    "    print(f\"Prompt learner for {few_shot}-shot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.mps.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on 1-shot CoOP CLIP: 51.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on 2-shot CoOP CLIP: 61.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on 4-shot CoOP CLIP: 71.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on 8-shot CoOP CLIP: 81.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on 16-shot CoOP CLIP: 83.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_accuracies = []\n",
    "for few_shot in [1,2,4,8,16]:\n",
    "    load_path = os.path.join(save_dir, f\"prompt_learner_{few_shot}_shot.pth\")\n",
    "    loaded_prompt_learner = SimplePromptLearner(clip_model, MO, n_ctx=16, ctx_init=None).to(device)\n",
    "    loaded_prompt_learner.load_state_dict(torch.load(load_path, map_location=device))\n",
    "\n",
    "    prompts = loaded_prompt_learner()\n",
    "    text_encoder = TextEncoder(clip_model=clip_model).to(device)\n",
    "\n",
    "    batch_accuracies = []\n",
    "    for image_paths, labels in tqdm(test_dataloader, leave=False, desc=f\"Testing {few_shot}-shot classifier\"):\n",
    "        with torch.no_grad():\n",
    "            test_images = [Image.open(img_path).convert(\"RGB\") for img_path in image_paths]\n",
    "            test_inputs = clip_processor(images=test_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "\n",
    "            text_features = text_encoder(prompts)\n",
    "            image_features = clip_model.get_image_features(**test_inputs)\n",
    "            \n",
    "            logit_scale = clip_model.logit_scale.exp()\n",
    "            test_logits = image_features @ text_features.T\n",
    "            test_logits *= logit_scale\n",
    "            \n",
    "            predictions = torch.argmax(test_logits, dim=1)\n",
    "            accuracy = (predictions == labels).float().mean().item()\n",
    "        batch_accuracies.append(accuracy)\n",
    "    print(f\"Test Accuracy on {few_shot}-shot CoOP CLIP: {np.mean(batch_accuracies) * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49408, 512])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.text_model.embeddings.token_embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 22, 512])\n",
      "<|startoftext|> within</w> syndrome</w> cos fooled</w> hot</w> ludo fathers</w> graci vy</w> louder</w> minute azar</w> poche rau havent</w> sbball</w> annual</w> crop</w> .</w> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> thenight</w> deals</w> famili le increased</w> bet</w> auditions</w> openings</w> monetary</w> blanco</w> damon uka</w> demon</w> fallen</w> yelled</w> iom</w> forest</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> fields</w> prince ðŁĩ®ðŁĩ³ asen welcoming</w> opar pist tyran âĻ¡</w> ðŁĹ£</w> sexually</w> piles</w> output</w> atism</w> invite</w> shiva herb aceous</w> vegetation</w> .</w> <|endoftext|>\n",
      "<|startoftext|> gate</w> blazing</w> ¨ babe</w> ancing</w> complex</w> âĿ¤ï¸ı enough</w> liberals</w> dra gosling</w> mean</w> low</w> langley</w> cept</w> clearly</w> highway</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> strata</w> ag</w> progress</w> passes</w> exploit</w> atlantis</w> characteristics</w> contained</w> structure</w> lacking</w> building office</w> structure</w> buildings</w> carriers</w> af industrial</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> oly</w> link</w> handle laugh</w> tweetapicture maxi megam together</w> melo marine</w> morning</w> strugg depu waited</w> mid capped</w> pasture</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> masters</w> isles</w> scenery</w> osaka</w> augh</w> philadelphia</w> labyrin finally</w> ley</w> pick</w> oasis</w> peanu cycles</w> bour anne</w> stray</w> permanent</w> crop</w> .</w> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> nach</w> lyk</w> ruler</w> clicked</w> meta bp legacy shepherd</w> ccl</w> hill</w> ola dt</w> shealth</w> raje dat</w> \"@_</w> residential</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> ancing</w> oux</w> aat off pll</w> nail</w> bakhtawar grow</w> ding</w> jeb</w> tha</w> turning</w> ering</w> castle</w> alt</w> urve river</w> .</w> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<|startoftext|> sieg barron</w> snowflakes</w> lek</w> abraham</w> clown</w> tracey</w> allstars</w> ouk</w> princess</w> pharma</w> bolton</w> umi</w> satu</w> bulaga</w> recruitment</w> sea</w> or</w> lake</w> .</w> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def decode_prompt(prompt_learner, clip_model, tokenizer):\n",
    "    # Get the optimized context embeddings\n",
    "    ctx_vectors = prompt_learner.ctx  # Shape: (num_classes, n_ctx, dim)\n",
    "    prompt = prompt_learner()\n",
    "    print(prompt.shape)\n",
    "    \n",
    "    # Get all possible token embeddings from CLIP's vocabulary\n",
    "    # vocab_size = clip_model.token_embedding.weight.shape[0]  # Total vocab size\n",
    "    token_embeddings = clip_model.text_model.embeddings.token_embedding.weight  # Shape: (vocab_size, dim)\n",
    "\n",
    "    # Find the closest tokens for each context vector\n",
    "    decoded_prompts = []\n",
    "    for class_idx in range(prompt.shape[0]):  # Iterate over classes\n",
    "        class_prompt = []\n",
    "        for ctx_token in prompt[class_idx]:  # Iterate over context words\n",
    "            similarities = cosine_similarity(ctx_token.unsqueeze(0), token_embeddings)  # Cosine similarity with vocab\n",
    "            closest_token_id = similarities.argmax().item()  # Get most similar token\n",
    "            closest_token = tokenizer.convert_ids_to_tokens([closest_token_id])[0]  # Convert to text\n",
    "            class_prompt.append(closest_token)\n",
    "\n",
    "        decoded_prompts.append(\" \".join(class_prompt))  # Reconstruct approximate text prompt\n",
    "\n",
    "    return decoded_prompts\n",
    "\n",
    "# Example Usage\n",
    "load_path = os.path.join(save_dir, f\"prompt_learner_16_shot.pth\")\n",
    "loaded_prompt_learner = SimplePromptLearner(clip_model, modified_classes, n_ctx=16, ctx_init=None).to(device)\n",
    "loaded_prompt_learner.load_state_dict(torch.load(load_path, map_location=device))\n",
    "decoded_prompts = decode_prompt(loaded_prompt_learner, clip_model, tokenizer)\n",
    "for class_prompt in decoded_prompts:\n",
    "    print(class_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.5461,  4.6747, 13.0687,  7.8658,  9.7408,  8.1585, 10.2043,  5.1856,\n",
      "          9.5310,  8.7160]], device='mps:0')\n",
      "tensor([[2.5066e-02, 1.9206e-04, 8.4902e-01, 4.6703e-03, 3.0453e-02, 6.2581e-03,\n",
      "         4.8407e-02, 3.2013e-04, 2.4689e-02, 1.0929e-02]], device='mps:0')\n",
      "tensor([2], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "load_path = os.path.join(save_dir, f\"prompt_learner_{few_shot}_shot.pth\")\n",
    "loaded_prompt_learner = SimplePromptLearner(clip_model, modified_classes, n_ctx=16, ctx_init=None).to(device)\n",
    "loaded_prompt_learner.load_state_dict(torch.load(load_path, map_location=device))\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompts = loaded_prompt_learner()\n",
    "    text_encoder = TextEncoder(clip_model=clip_model).to(device)\n",
    "    test_images = [Image.open(\"2750/HerbaceousVegetation/HerbaceousVegetation_2.jpg\").convert(\"RGB\")]\n",
    "    test_inputs = clip_processor(images=test_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "\n",
    "    text_features = text_encoder(prompts)\n",
    "    image_features = clip_model.get_image_features(**test_inputs)\n",
    "    \n",
    "    logit_scale = clip_model.logit_scale.exp()\n",
    "    test_logits = image_features @ text_features.T\n",
    "    # test_logits *= logit_scale\n",
    "    print(test_logits)\n",
    "    print(torch.softmax(test_logits,dim=-1))\n",
    "    \n",
    "    predictions = torch.argmax(test_logits, dim=1)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EuroSAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
