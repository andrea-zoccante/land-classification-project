{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/EuroSAT/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import config\n",
    "from customCLIP import customCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded coop model from models/prompt_learners/coop/8_shot.pth\n",
      "Testing in coop mode\n",
      "tensor([1], device='mps:0') tensor([[0.9504]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "FEW_SHOT = 8\n",
    "MODE = \"coop\"\n",
    "IMAGE_PATH = \"2750/Forest/Forest_109.jpg\"\n",
    "\n",
    "customclip = customCLIP(model_name=\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "if MODE == \"coop\":\n",
    "    model_path = f\"models/prompt_learners/coop/{FEW_SHOT}_shot.pth\"\n",
    "    customclip.load_model(model_path, mode=MODE)\n",
    "elif MODE in [\"linear_probe\", \"MLP_probe\", \"logreg_probe\"]:\n",
    "    model_path = f\"results/classifiers/{MODE}/{FEW_SHOT}-shot\"\n",
    "    customclip.load_model(model_path, mode=MODE)\n",
    "elif MODE == \"zeroshot\":\n",
    "    pass\n",
    "\n",
    "customclip.set_testing_mode(MODE)\n",
    "prediction, prob = customclip.classify_images_clip([IMAGE_PATH])\n",
    "print(prediction.item(), prob.item())\n",
    "# print(customclip.class_labels[prediction.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EuroSAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
